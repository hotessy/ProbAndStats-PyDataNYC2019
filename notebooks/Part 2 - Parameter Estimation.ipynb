{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation (and Hypothesis testing... which is just parameter estimation)\n",
    "\n",
    "In our `MagicCoin` example `n` was an example of a parameter. It's some important value for our understanding of how things work that we don't know. By using probability and data we can come up with ways to estimate what these parameters might be.\n",
    "\n",
    "In the rest of this tutorial we'll focus on an admittedly boring case of trying to estimate the rate that a product sells. The rate being the parameter we're trying to estimate.\n",
    "\n",
    "We'll be using a simulated product and customer data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context import src\n",
    "from src import customer as cust\n",
    "from src import product as prod\n",
    "from src import experiment as exp\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a new product. Our `Product` class is amazingly simple. It just takes a `name`, a `price` and a `quality`. Of course `quality` of a product is not so easy, but in this case we can just set it to whatever we want to simulate the average reviews are.\n",
    "\n",
    "We'll start with a toothbrush:\n",
    "\n",
    "<img src=\"./images/tooth_brush.jpg\" alt=\"A toothbrush\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toothbrush = prod.Product(name=\"alright brush\",\n",
    "                          price=4.99,\n",
    "                          quality=3.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not the most amazing toothbrush, but let's try to sell it to somebody. Our `Customer` class can generate a random customer for us that has a certain price threshold and quality threshold that we don't know, and varies from customer to customer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_a = cust.Customer.get_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can show our customer the product using the `will_purchase` method and we'll get a bool back telling us whether or not the decided to purchase that product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_a.will_purchase(toothbrush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the `MagicCoin` we don't really know what's going on inside of our customer. This singular observation doesn't tell us very much about how our coin behaves. The best way to handle this is to run some sort of an `Experiment`!\n",
    "\n",
    "We can create an experiment using our `Experiment` class which will help us collect data on customers that we've shown our `toothbrush` to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toothbrush_test = exp.Experiment(toothbrush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our experiment to show this `toothbrush` to a bunch of customers and see how many will purchase it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = toothbrush_test.show_to_customers(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how our test went..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.purchased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `MagicCoin` we understood exactly how it worked so we didn't need a model to represent how we think the `ModelCoin` worked.\n",
    "\n",
    "Even though the interaction between the `Customer` and the `Product` is more complicated that simply a probability that a `Customer` will purchase a `Product` from our view the best way to model this is to make the simplifying assumption that each `Product` sells on at a particular rate.\n",
    "\n",
    "If you've worked in ecommerce or any similar conversion focused industry it might seem obvious that we want to estimate \"the rate that a product sells\", but it's good to remember that this is really an application of probability. If we knew everything about each customer and each product and how they interact we would know exactly how a product sells, there would be no rate involved at all!\n",
    "\n",
    "So if you wanted to estimate a rate at what this product sells, what would be your first estimate? Most people intuitively might estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold = sum(test_result.purchased)\n",
    "total = len(test_result.purchased)\n",
    "rate_est = sold/total\n",
    "print(\"{0} sold out of {1} total so our estimate is {2}\".format(sold,total,rate_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making our `rate_est` as `sold/total` seems like a pretty good idea. But how can we show this is the best estimate, and are there other good estimate?\n",
    "\n",
    "For example what about `rate_est + 0.05` or  `rate_est - 0.05`? Are these good estimate? certainly they explain the data pretty well. How would we compare these estimates?\n",
    "\n",
    "One way we can do this is to use the Binomial distribution. The Binomial distribution will tell us exactly how likely data would be given this was the rate of a product selling. Let's look at the probability of the data for each of these alternate options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_est = stats.binom(n=total,p=rate_est)\n",
    "bin_est.pmf(sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_est_bit_smaller = stats.binom(n=total,p=rate_est-0.05)\n",
    "bin_est_bit_smaller.pmf(sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_est_bit_bigger = stats.binom(n=total,p=rate_est+0.05)\n",
    "bin_est_bit_bigger.pmf(sold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we can see that the slightly different estimates are a bit more surprised by the data that we observed. This means that they are not quite a good of an explaination of the data.. but that doesn't mean they're *wrong*.\n",
    "\n",
    "If you flipped a coin 3 times and got 1 head, you wouldn't assume the probability of heads is $\\frac{1}{3}$ the coin being fair is still pretty likely and you know that most coins tend to be fair so the belief that the probability of heads is $\\frac{1}{2}$\n",
    "\n",
    "Rather than just worry about which estimate for the rate is the best, it might be a good idea to look at how strongly we believe in multi estimates. We could start by looking at estimates in intervals of every 0.05 and apply the same logic using the Binomial Distribution we did before. We can use `numpy` to quickly do this in a vectorized way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_dists = stats.binom(total,np.arange(0,1,0.1))\n",
    "sns.lineplot(x=np.arange(0,1,0.1),\n",
    "              y=est_dists.pmf(sold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that if we look at each possibility between 0 and 1 incrementing by 0.05 we have some sort of distribution forming. \n",
    "\n",
    "We can see that this continues to smooth as we shrink our increment size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_dists = stats.binom(total,np.arange(0,1,0.05))\n",
    "sns.lineplot(x=np.arange(0,1,0.05),\n",
    "              y=est_dists.pmf(sold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_dists = stats.binom(total,np.arange(0,1,0.01))\n",
    "sns.lineplot(x=np.arange(0,1,0.01),\n",
    "              y=est_dists.pmf(sold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Beta Distribution\n",
    "\n",
    "The distribution we're converging on is a very useful distribution call the *Beta distribution*. It differs from our plot above in two ways: First it is a continous distribution meaning it accounts for the infinitely many possible rates for what we've observed. The second is that it makes it so that if we sum up over all those possible points (technically integrate) the result is exactly 1. This let's us talk about probabilities for different values. \n",
    "\n",
    "The Beta distribution takes two paramters $\\alpha$ the number of successes or `True` values we oberved and $\\beta$ the number failures or `False` values. Note that this is bit different than the Binomial where `n` = `alpha+beta`\n",
    "\n",
    "Here's a plot of what this distribution looks like for our cases of `alpha = sold` and `beta = total - sold`\n",
    "\n",
    "**note:** because the Beta distribution is continuous we'll use the `.pdf` method rather than the `.pmf` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = sold\n",
    "beta = total - sold\n",
    "est_beta_dist = stats.beta(alpha,beta)\n",
    "xs = np.arange(0,1,0.01)\n",
    "sns.lineplot(x=xs,\n",
    "             y=est_beta_dist.pdf(xs)).set(xlabel='rate estimate', \n",
    "                                          ylabel='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a plot that shows the distribution of how strongly we believe in various possible rates for our `toothbrush` to sell to a custuomer.\n",
    "\n",
    "The power of having a distribution like this is that we use it to ask questions about our beliefs.\n",
    "\n",
    "For example: suppose the director of marketing came in an asked if you had a product she could feature on the company webiste. The catch is she wants a product that will have a rate of *at least* 0.4.\n",
    "\n",
    "Looking at the distribution of our beliefs it's certainly possible that it could be at least 0.4, but how confindent are we?\n",
    "\n",
    "We can answer this question a few ways. One way would be to use calculus to integrate between 0.4 and 1.0. But plenty of people are a bit nervous about doing calculus, and it turns out that integration gets tricky no matter what, so what we can also do is simply sample from this distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "rate_est_samples = np.random.beta(a=sold,b=(total-sold),size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a vector of samples of our rate estimate and we can use them to answer questions like the directors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rate_est_samples > 0.4)/n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we compute the `mean` of these `rate_est_samples` get a result very similar to `sold/total`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_est_samples.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no coincidence. It turns out that the analytical *expectation* (or mean) of the Beta distribution is \n",
    "\n",
    "$$E[Beta(\\alpha,\\beta)] = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
    "\n",
    "Which is the same as the successes over the total. If we sample more from out distribution the answers to the questions we ask of those samples are going to be closer and to the answer we would get if we perform the correct mathematical operations on our function. This is super important because it means we can use sampling as a subsitute for increasingly complicated integrals.\n",
    "\n",
    "So if you aren't super comfortable with the math, definitely feel free to just use sample... and even if you *are* comfortable with the math, pretty soon you'll need to rely on sampling techniques anyway so you should start playing around with sampling in your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing: comparing two products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing is the essential parts of statistics. It's fancy way of saying you have some hypothesis about the data and you want to test out how likely that hypothesis is. In fact, we've already done hypthesis testing: we asked about the hypothesis of each possible rate that could explain the data, and also looked at the hypothesis that that rate is greater than 0.4\n",
    "\n",
    "But typically when people think of hypothesis tests they think of comparing some things like:\n",
    "\n",
    "- do patients that recieve the treatment get healthy faster\n",
    "- is the new version of the website have more sign-ups than the old\n",
    "- does saying \"Florida\" make people think of being old and then walk slower than people who don't hear it.\n",
    "\n",
    "So to explore hypothesis testing we're going to have to have something to compare our `tooth brush` with, which is our `luxury_toothbrush`\n",
    "\n",
    "<img src=\"./images/luxury_toothbrush.jpg\" alt=\"A luxury toothbrush\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luxury_toothbrush = prod.Product(name=\"luxury toothbrush\",\n",
    "                                 price=7.99,\n",
    "                                 quality=4.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better toothbrush, but it also costs more, how well will it do? And more important, how well will it do compared to just our regular `toothbrush`? \n",
    "\n",
    "To look at this let's set up a new experiment, this time one where we can compare both toothbrushes to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toothbrush_ab_test = exp.Experiment(toothbrush,luxury_toothbrush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're calling this an AB test because it's similar in nature to an AB test for websites. We're going to show each variant, the original `toothbrush` and the `luxury_toothbrush` to different groups of people and see which one does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ab_test_samples = 30\n",
    "ab_test_results = toothbrush_ab_test.show_to_customers(n_ab_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now showed each toothbrush to 30 different people here's the results for each tooth brush"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `toothbrush` we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test_results[['a_purchased']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for `luxury_toothbrush`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_test_results[['b_purchased']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sold = sum(ab_test_results.a_purchased)\n",
    "b_sold = sum(ab_test_results.b_purchased)\n",
    "print(\"A sold {} (rate {:.2f}) and B sold {} (rate {:.2f})\".format(\n",
    "    a_sold, float(a_sold / n_ab_test_samples), b_sold,\n",
    "    float(b_sold / n_ab_test_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which toothbrush is beter? And if you think you're sure rerun these cells a few times and you're likely to get different results!\n",
    "\n",
    "To better understand what's happening here look at our parameter estimates for each tooth brush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_a = sum(ab_test_results.a_purchased)\n",
    "beta_a = n_ab_test_samples - alpha_a \n",
    "a_beta_dist = stats.beta(alpha_a,beta_a)\n",
    "\n",
    "alpha_b = sum(ab_test_results.b_purchased)\n",
    "beta_b = n_ab_test_samples - alpha_b \n",
    "b_beta_dist = stats.beta(alpha_b,beta_b)\n",
    "rates = np.arange(0,0.5,0.005)\n",
    "plot_df = pd.DataFrame({\n",
    "    'density':np.concatenate((a_beta_dist.pdf(rates),\n",
    "                              b_beta_dist.pdf(rates))),\n",
    "    'rate': np.concatenate((rates,rates)),\n",
    "    'group':['regular']*len(rates) + ['luxury']*len(rates)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='rate',\n",
    "             y='density',\n",
    "             hue='group',\n",
    "             data=plot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can se that we have two estimates now that might tell us very different things. If you look at your neighbors' plots (if you're in the live workshop) you might notice a very different plot based on which customers looked at the times.\n",
    "\n",
    "this means that we don't have enough data to tell which distribution is different. What if we had more customers?\n",
    "\n",
    "Let's try 100 customers for each brush and see what we learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ab_test_2_samples = 100\n",
    "ab_test_2 = toothbrush_ab_test.show_to_customers(n_ab_test_2_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that we're over writing the variables here so be careful of what order you run these cells in!\n",
    "alpha_a = sum(ab_test_2.a_purchased)\n",
    "beta_a = n_ab_test_2_samples - alpha_a \n",
    "a_beta_dist = stats.beta(alpha_a,beta_a)\n",
    "\n",
    "alpha_b = sum(ab_test_2.b_purchased)\n",
    "beta_b = n_ab_test_2_samples - alpha_b \n",
    "b_beta_dist = stats.beta(alpha_b,beta_b)\n",
    "rates = np.arange(0,0.5,0.001)\n",
    "plot_df = pd.DataFrame({\n",
    "    'density':np.concatenate((a_beta_dist.pdf(rates),\n",
    "                              b_beta_dist.pdf(rates))),\n",
    "    'rate': np.concatenate((rates,rates)),\n",
    "    'group':['regular']*len(rates) + ['luxury']*len(rates)\n",
    "})\n",
    "sns.lineplot(x='rate',\n",
    "             y='density',\n",
    "             hue='group',\n",
    "             data=plot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting much better results! There's still a lot of uncertainity around exactly what rate each toothbrush sells, but we can clearly see that our estimates for `luxury_toothbrush` are much lower than they are for regular. Even if you look at your neighbors' plots this time, they should look much more similar.\n",
    "\n",
    "But we might want to quantify *exactly* how certain we are that the `toothbrush` is doing better than the `luxury_toothbrush`. The best way to do that is with sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 10000\n",
    "regular_samples = np.random.beta(a=alpha_a,b=beta_a,size=total_samples)\n",
    "luxury_samples = np.random.beta(a=alpha_b,b=beta_b,size=total_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will tell the probability that the regular toothbrushes sell better than luxury:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(regular_samples >luxury_samples)/total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we are almost certain that this is the case, even though we have a lot different beliefs about how well the different toothbrushes convert.\n",
    "\n",
    "Just to be clear, we just did a hypothesis test! The **hypothesis** was that variant A was better than variant B, and the result of that test was the probability we got from our simulation. The great thing about this test is we don't have a \"p value\" we have the actual probability that the `toothbrush` is superior based on our simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Here are some things you can experiment around with on your own to get a better sense of how hypothesis tests work!\n",
    "\n",
    "We can see that the `luxury_toothbrush` sells at a lower rate than the `toothbrush` but there are still many questions we can ask. How *much* better do we think `toothbrush` is than `luxury_toothbrush` in terms of how many times better it converts?\n",
    "\n",
    "In [this post on the Count Bayesie blog](https://www.countbayesie.com/blog/2015/4/25/bayesian-ab-testing) I go over ways that you can add a prior probability (and in [this post](https://www.countbayesie.com/blog/2015/2/18/hans-solo-and-bayesian-priors) talk a bit about what prior probabilities are). Experiment with using a reasonable prior probability for the different purchase rates and see how much sooner (or longer) it takes to conclude that `luxury_toothbrush` is inferior.\n",
    "\n",
    "\n",
    "`luxury_toothbrush` sells at a lower rate, but it also is more expensive. When we take into account the price difference with the sales difference is the plain old `toothbrush` the better product? or would we be better off selling less of the better tooth brush?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
